{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 로딩\n",
    "\n",
    "샘플로 1000 개의 뉴스 기사만 이용합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "965"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../../tutorial_data/')\n",
    "\n",
    "from data_loader import get_news_corpus_as_list\n",
    "\n",
    "docs = get_news_corpus_as_list(n_docs=1000)\n",
    "docs = [doc for doc in docs if doc]\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build bigram dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from konlpy.tag import Komoran\n",
    "\n",
    "komoran = Komoran()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bigram_extractor 는 min_count 이상인 bigram 만을 선택하는 extractor 입니다. \n",
    "\n",
    "to_bigram 은 두 개의 list 를 zip 으로 묶어서 bigram list 을 만드는 함수입니다. \n",
    "\n",
    "bigram_counter 로 카운팅을 한 뒤, min_count 이상인 bigram 만을 선택하여 bigram_dictionary 를 만듭니다. 총 2835 개의 bigram 을 선택하였습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2835"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def bigram_extractor(docs, min_count=10):\n",
    "\n",
    "    def to_bigram(tokens):\n",
    "        bigrams = [(t0, t1) for t0, t1 in zip(tokens, tokens[1:])]\n",
    "        return bigrams\n",
    "\n",
    "    bigram_counter = Counter(\n",
    "        [bigram for doc in docs for bigram in\n",
    "         to_bigram(komoran.pos(doc, join=True)) if doc\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    bigram_dictionary = {\n",
    "        bigram:count for bigram, count in bigram_counter.items()\n",
    "        if count >= min_count\n",
    "    }\n",
    "\n",
    "    return bigram_dictionary\n",
    "\n",
    "bigrams = bigram_extractor(docs)\n",
    "len(bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "임의로 다섯개의 bigram 을 살펴봅니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('서울/NNP', '연합뉴스/NNP'),\n",
       " ('연합뉴스/NNP', '경찰/NNG'),\n",
       " ('경찰/NNG', '관계자/NNG'),\n",
       " ('관계자/NNG', '들/XSN'),\n",
       " ('들/XSN', '이/JKS')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bigrams)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bigram tokenizer\n",
    "\n",
    "BigramTokenizer 는 base tokenizer 인 tagger 와 bigrams 를 입력받는 class 입니다. \n",
    "\n",
    "\\_\\_call\\_\\_() 함수를 구현하면 클래스도 함수처럼 호출할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramTokenizer:\n",
    "\n",
    "    def __init__(self, bigrams, tagger):\n",
    "        self.bigrams = bigrams\n",
    "        self.tagger = tagger\n",
    "\n",
    "    def __call__(self, sent):\n",
    "        if not sent:\n",
    "            return []\n",
    "\n",
    "        unigrams = self.tagger.pos(sent, join=True)\n",
    "\n",
    "        bigrams = [(t0, t1) for t0, t1 in zip(unigrams, unigrams[1:])]\n",
    "        bigrams = [bigram for bigram in bigrams if bigram in self.bigrams]\n",
    "        bigrams = ['%s-%s' % (t0, t1) for t0, t1 in bigrams]\n",
    "\n",
    "        return unigrams + bigrams\n",
    "\n",
    "bigram_tokenizer = BigramTokenizer(bigrams, komoran)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "샘플로 하나의 문장을 선택하여 토크나이징을 합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'오패산터널 총격전 용의자 검거 서울 연합뉴스 경찰 관계자들이 19일 오후 서울 강북구 오패산 터널 인근에서 사제 총기를 발사해 경찰을 살해한 용의자 성모씨를 검거하고 있다 성씨는 검거 당시 서바이벌 게임에서 쓰는 방탄조끼에 헬멧까지 착용한 상태였다 독자제공 영상 캡처 연합뉴스'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = docs[1].split('  ')[0]\n",
    "sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bigram_tokenizer(sent) 로 클래스를 호출합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오패산터널/NNP',\n",
       " '총격전/NNG',\n",
       " '용의자/NNP',\n",
       " '검거/NNG',\n",
       " '서울/NNP',\n",
       " '연합뉴스/NNP',\n",
       " '경찰/NNG',\n",
       " '관계자/NNG',\n",
       " '들/XSN',\n",
       " '이/JKS',\n",
       " '19/SN',\n",
       " '일/NNB',\n",
       " '오후/NNG',\n",
       " '서울/NNP',\n",
       " '강북구/NNP',\n",
       " '오/NNP',\n",
       " '패사/NNG',\n",
       " 'ㄴ/JX',\n",
       " '터널/NNP',\n",
       " '인근/NNG',\n",
       " '에서/JKB',\n",
       " '사제/NNP',\n",
       " '총기/NNG',\n",
       " '를/JKO',\n",
       " '발사/NNG',\n",
       " '하/XSV',\n",
       " '아/EC',\n",
       " '경찰/NNG',\n",
       " '을/JKO',\n",
       " '살해/NNG',\n",
       " '하/XSV',\n",
       " 'ㄴ/ETM',\n",
       " '용의자/NNP',\n",
       " '성모/NNP',\n",
       " '씨/NNB',\n",
       " '를/JKO',\n",
       " '검거/NNG',\n",
       " '하/XSV',\n",
       " '고/EC',\n",
       " '있/VV',\n",
       " '다/EC',\n",
       " '성씨/NNP',\n",
       " '는/JX',\n",
       " '검거/NNG',\n",
       " '당시/NNG',\n",
       " '서바이벌/NNP',\n",
       " '게임/NNG',\n",
       " '에서/JKB',\n",
       " '쓰/VV',\n",
       " '는/ETM',\n",
       " '방탄/NNP',\n",
       " '조끼/NNP',\n",
       " '에/JKB',\n",
       " '헬멧/NNP',\n",
       " '까지/JX',\n",
       " '착용/NNG',\n",
       " '하/XSV',\n",
       " 'ㄴ/ETM',\n",
       " '상태/NNG',\n",
       " '이/VCP',\n",
       " '었/EP',\n",
       " '다/EC',\n",
       " '독자/NNG',\n",
       " '제공/NNG',\n",
       " '영상/NNP',\n",
       " '캡처/NNP',\n",
       " '연합뉴스/NNP',\n",
       " '서울/NNP-연합뉴스/NNP',\n",
       " '연합뉴스/NNP-경찰/NNG',\n",
       " '경찰/NNG-관계자/NNG',\n",
       " '관계자/NNG-들/XSN',\n",
       " '들/XSN-이/JKS',\n",
       " '이/JKS-19/SN',\n",
       " '19/SN-일/NNB',\n",
       " '일/NNB-오후/NNG',\n",
       " '오후/NNG-서울/NNP',\n",
       " '서울/NNP-강북구/NNP',\n",
       " '강북구/NNP-오/NNP',\n",
       " '오/NNP-패사/NNG',\n",
       " '패사/NNG-ㄴ/JX',\n",
       " 'ㄴ/JX-터널/NNP',\n",
       " '터널/NNP-인근/NNG',\n",
       " '인근/NNG-에서/JKB',\n",
       " '에서/JKB-사제/NNP',\n",
       " '사제/NNP-총기/NNG',\n",
       " '총기/NNG-를/JKO',\n",
       " '를/JKO-발사/NNG',\n",
       " '발사/NNG-하/XSV',\n",
       " '하/XSV-아/EC',\n",
       " '살해/NNG-하/XSV',\n",
       " '하/XSV-ㄴ/ETM',\n",
       " '씨/NNB-를/JKO',\n",
       " '를/JKO-검거/NNG',\n",
       " '검거/NNG-하/XSV',\n",
       " '하/XSV-고/EC',\n",
       " '고/EC-있/VV',\n",
       " '있/VV-다/EC',\n",
       " '다/EC-성씨/NNP',\n",
       " '성씨/NNP-는/JX',\n",
       " '쓰/VV-는/ETM',\n",
       " '방탄/NNP-조끼/NNP',\n",
       " '착용/NNG-하/XSV',\n",
       " '하/XSV-ㄴ/ETM',\n",
       " 'ㄴ/ETM-상태/NNG',\n",
       " '상태/NNG-이/VCP',\n",
       " '이/VCP-었/EP',\n",
       " '었/EP-다/EC',\n",
       " '독자/NNG-제공/NNG',\n",
       " '제공/NNG-영상/NNP',\n",
       " '영상/NNP-캡처/NNP',\n",
       " '캡처/NNP-연합뉴스/NNP']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_tokenizer(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이를 CountVectorizer 의 tokenizer 로 입력합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=bigram_tokenizer)\n",
    "x = vectorizer.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "총 18,835 개의 uni + bigram 으로 term frequency vector 가 만들어집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(965, 18835)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. All bigrams\n",
    "\n",
    "sklearn.feature_extraction.text.CountVectorizer 의 ngram_range=(1, 2) 로 설정한다면, 가능한 모든 종류의 bigram 을 term 으로 이용하게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def all_bigram_tokenizer(sent):\n",
    "    if not sent:\n",
    "        return []\n",
    "    return komoran.pos(sent, join=True)\n",
    "\n",
    "vectorizer_ = CountVectorizer(\n",
    "    ngram_range=(1,2),\n",
    "    tokenizer=all_bigram_tokenizer\n",
    ")\n",
    "\n",
    "x_ = vectorizer_.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "109,481 개의 uni + bigram 을 이용하여 term frequency vector 를 만듭니다. ngram_range 를 이용할 때에는 min_df, max_df 를 이용하여 infrequent bigram 을 제거해야 적절한 크기의 차원을 유지할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(965, 109481)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
